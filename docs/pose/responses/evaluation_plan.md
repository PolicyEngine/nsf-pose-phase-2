### Outcomes and Metrics
We will measure what matters to an ecosystem: adoption, contribution, quality and security, and calibration performance. Adoption is tracked through named deployments, monthly active users, and simple retention measures that show whether organizations keep the system in use. Contribution is captured by external pull requests and issues, the time it takes to review and merge changes, and the number of active community maintainers. Quality and security are monitored by test coverage and reliability checks, the time to remediate vulnerabilities, and the share of releases that are signed and include a SBOM; reproducibility audits make sure published numbers can be rerun. For calibration work, we report the share of targets within tolerance—for example within ten percent—the loss trajectory over epochs, stability across holdout splits, and, when we exclude targets, why we chose to do so.

### Measurement Plan
We will baseline these metrics at the outset and present them on a public dashboard. Each quarter the steering group reviews the indicators and the evidence from I‑Corps interviews and observed usage, then adjusts the roadmap accordingly. The validation harness runs in CI and publishes signed scorecards; for state and congressional district weighting runs, we retain epoch‑level logs and share summaries of loss reduction and target coverage so partners can judge progress at a glance. The guiding idea is to make our own learning loop visible: when adoption falters, we see it; when reviews slow, we see that too; when calibration and cross‑model agreement improve, the charts make the story legible to reviewers and collaborators.
