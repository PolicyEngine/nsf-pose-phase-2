### Broader Impacts
This project broadens access to evidence‑based policymaking by delivering an auditable, open‑source platform for distributional and budget analysis that researchers, policymakers, journalists, and the public can use and improve. In practice, this means that a legislative office can test a change to a credit and see its incidence; a newsroom can reproduce the analysis behind a headline; and an academic seminar can dig into the assumptions and extend the work. By reducing dependence on proprietary models, we make debates easier to adjudicate on the merits and help good ideas travel further, faster.

Our broader‑impacts activities focus on practical steps that make participation and use easier. Contributor pathways come with mentorship, first‑timer issues, and clear documentation so new maintainers can confidently make their first change. Training and outreach run through workshops, case studies, and partner pilots in academia and government, so the tooling is taught where it will be used. Privacy‑preserving workflows and guidance keep sensitive microdata out of repositories and show analysts how to work safely and lawfully with local systems.

Over the award we expect to see wider adoption of reproducible policy analysis, wider access to high‑quality tools, a steady increase in external contributions and maintainers, and clearer transparency around modeling assumptions and results. We will track adoption, contribution, and quality/security alongside calibration metrics, and we will adjust activities based on evidence from I‑Corps interviews and observed usage, making our learning process as open as our code.

### Illustrative Use Cases
Consider three common settings. A state revenue office is evaluating a refundable credit redesign; an analyst loads a canonical scenario, toggles eligibility parameters, and immediately sees distributional tables by decile, geography, and family type, with footnotes that cite assumptions and links to reproducible notebooks. A newsroom needs to sanity‑check a claim about the budget effect of a late‑breaking amendment; an editor opens a saved scenario, updates the effective date and income ranges, and exports ready‑to‑publish charts, with the methodology available in the references. A benefit navigator wants to estimate the take‑up and cash‑flow effect of a policy change for their clients; a program lead imports a county‑level scenario, runs head‑to‑head comparisons against baseline and published scores, and shares results with a case‑worker team. In each case, the path from question to reviewed answer is short, reproducible, and public—exactly the behavior an OSE should enable.
